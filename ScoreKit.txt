

--- Page 1 ---

ScoreKit: Unifying Notation and Real-Time
Rendering in Fountain-Coach
Overview of ScoreKit and AudioTalk Vision
ScoreKit is  a  proposed  Swift  library  that  bridges  high-quality  musical  notation  engraving  with
interactive, real-time score rendering. It is conceived as a crucial component of the Fountain-Coach
ecosystem to realize the AudioTalk vision, where users can “talk about sound” in natural language and
see those descriptions translated into musical notation and expressive playback. In essence,
ScoreKit would wrap two key capabilities: (A) the  LilyPond engraving engine for professional sheet
music output, and (B) a Swift-native score rendering engine inspired by Verovio for dynamic, on-the-fly
visualization. By integrating with FountainAI, Fountain-Store, MIDI2, and Teatro, ScoreKit enables a
seamless loop from semantic musical ideas to notation and sound, making  “speak music, hear it
happen” more than just a tagline.
LilyPond Integration for Engraving
LilyPond is treated as the  source of truth for notation in AudioTalk, due to its proven ability to
produce publication-quality scores. ScoreKit will provide a Swift wrapper around LilyPond, allowing the
system to generate or update sheet music programmatically. In the existing Teatro prototype, a simple
LilyScore struct  already  demonstrates  this  approach  by  writing  a  .ly file  and  invoking  the
lilypond command-line tool to produce PDF output. ScoreKit would build on this by handling
LilyPond sessions more robustly – for example, managing temporary files, asynchronous rendering, and
error capture (so FountainAI can be alerted to notation mistakes). The LilyPond integration ensures any
musical idea or AI-generated fragment can be typeset into standard notation and exported (PDF, SVG,
etc.) for printing or analysis. 
Environment requirements: LilyPond must be installed on the host (or accessible via a service) for
ScoreKit to call it. On Apple platforms like iOS where bundling GPL software is problematic,
ScoreKit can fall back to native solutions (e.g. using PDFKit to draw notation). In all cases, the
LilyPond .ly source remains the authoritative representation that can be saved, versioned, or
shared. Notational edits requested via AudioTalk (e.g. “add a crescendo in bars 1–4”) will ultimately
reflect as LilyPond changes – either by generating corresponding LilyPond syntax or by updating
an internal model that is re-engraved to LilyPond format. This guarantees that the formal score
always stays aligned with the musical intent applied.
Swift Native Real-Time Score Rendering
While LilyPond provides beautiful output, it’s not designed for real-time interactivity. To give immediate
visual  feedback  and  cross-platform  support,  ScoreKit  includes  a  Swift-native  score  renderer re-
engineered from Verovio’s approach. Verovio is a lightweight engraving library (LGPL-licensed) that
renders music from formats like MEI or MusicXML into SVG rapidly. A Swift incarnation of this idea
would allow dynamic score updates within apps or web views without needing to rerun LilyPond for
every change. ScoreKit’s renderer could parse an internal score representation (or a format like MEI/
MusicXML derived from the LilyPond content) and draw notation using CoreGraphics or SwiftUI. This
1 2
3
4
5
6
• 
7
8
2
9
1

--- Page 2 ---

aligns with prior suggestions to use Apple’s PDFKit or native drawing instead of shelling out to LilyPond
on iOS . 
Capabilities: The real-time engine would handle layout of staves, notes, and symbols on the fly.
For example, when FountainAI applies a slur or dynamic marking via ScoreKit, the change would
immediately reflect on the displayed score view (e.g. a tie curve appears, a crescendo hairpin is
drawn) without waiting for a PDF regenerate. This interactive rendering also enables 
highlighting and animation – important for coaching or A/B comparisons. (Notably, the Teatro
engine already supports animated SVG outputs and storyboards; ScoreKit can extend
this to highlight sections being discussed or modified in real time.) If Verovio itself can be
wrapped or compiled for Swift, that’s an option, but a native implementation grants tighter
integration with Fountain-Coach’s custom format and future extensions. The Swift renderer
ensures that notation becomes responsive: it’s always in sync with the latest user or AI actions,
supporting an interactive “score editor” experience embedded in the Fountain ecosystem. 
Fountain-Coach Score Format and Semantic Annotations
To power AudioTalk’s  semantic control of music, ScoreKit would introduce a Fountain-Coach–specific
score format or data model that enriches traditional notation with  meaningful tags. This could be
realized as an extension of LilyPond or as a separate JSON/Swift struct representation that the system
uses internally. For example, LilyPond comments like %!AudioTalk: warm_timbre are proposed as
a way to embed timbral intent in the sheet music. ScoreKit can parse and produce these annotations
systematically.  Each  musical  element  (notes,  phrases,  dynamics)  in  the  ScoreKit  model  can  carry
metadata linking it to semantic descriptors or AudioTalk  macros. This fulfills the “semantic memory”
idea: the system remembers not just the notes, but the context and intent behind them.
Custom format: The format might include unique IDs for measures and phrases, allowing
FountainAI to reference them unambiguously (e.g. “increase brightness on phrase #ID42”). It
should capture structural data (phrases, sections, instrument assignments) alongside semantic
labels (like “shimmer pad effect” applied to a section). By having a consistent internal
representation, ScoreKit can output standard LilyPond for engraving, and serve data to other
components (e.g. to the AI or UI) without losing the semantic markers. This also makes it
possible to store and query the score in Fountain-Store (the memory core) by those tags or IDs.
For instance, the system could retrieve all past scores or passages tagged with “legato
crescendo” if needed, using Fountain-Store’s fast lookup by keywords. In short, ScoreKit’s
format bridges the gap between symbolic music data and the “knowledge” of what that music
means in coaching or creative terms.
MIDI 2.0 Integration and Audio Playback
A core purpose of ScoreKit is to connect the notated score with actual sound, leveraging  MIDI 2.0
(UMP) for high-resolution, expressive playback. When ScoreKit processes a score, it can generate a
corresponding stream of MIDI 2.0 events (notes and control changes) that reflect the notation exactly –
this is crucial for AudioTalk’s “notation ↔ performance bridge”. Rather than relying on LilyPond’s
own MIDI output (which is limited to MIDI 1.0), ScoreKit will translate notation into UMP messages in
real time. Every marking on the score can influence these messages: for example, a written crescendo
becomes a gradual increase in per-note velocity or volume controllers, and a  “brighter” annotation
might translate into MIDI 2.0 Per-Note Expression (PNX) data that raises a filter cutoff for those notes
8
• 
10 11
4
• 
12
2
2

--- Page 3 ---

. The rich resolution and per-note control of MIDI 2.0 means the playback can closely mirror the
nuance of the notation. 
midi2sampler: ScoreKit’s MIDI output will feed into the midi2sampler engine (and other synth
backends like Csound or SDLKit) to produce sound. midi2sampler is built to handle per-note
controllers, high-definition velocity, and other advanced features of MIDI 2.0, so when ScoreKit
sends a note with attached timbre/brilliance data, the sampler will actually alter the sound of
that note (e.g. crossfading to a brighter sample layer). Integration here means mapping LilyPond
instruments or articulations to specific MIDI presets or profiles – for instance, if the score
indicates a violin pizzicato, ScoreKit might ensure the MIDI event selects the correct sample zone
for that articulation. By working in tandem with the Audio engine, ScoreKit helps ensure that 
what you see is what you hear: the symbolic and acoustic representations stay aligned. This closes
the loop envisioned in AudioTalk where a user’s spoken request yields both an updated score and
an updated playback. As an example, when a user says “warm legato crescendo on bars 1–4”, the
system (via AudioTalk’s intent parsing) produces a plan to add a slur and crescendo marking in
LilyPond and to adjust timbre/volume in MIDI. ScoreKit would apply the slur and hairpin to the
score, and concurrently emit MIDI 2.0 events for those bars that gradually increase expression
and warmth, driving the sampler to realize the sound. 
Integration with FountainAI and Memory Systems
ScoreKit is designed to be orchestrated by FountainAI, the AI agent that mediates user requests and
system planning in Fountain-Coach. FountainAI can call into ScoreKit’s API to accomplish tasks like
composing a new score snippet, inserting a musical phrase, or applying an AudioTalk intent. Because
ScoreKit abstracts the low-level details, FountainAI can operate at a higher level – e.g., requesting “add
an 8-bar chord progression in G major” and letting ScoreKit generate the LilyPond structure for it. This
division of labor lets the AI focus on  what to create, while ScoreKit handles  how to implement it in
notation and data. Crucially, ScoreKit can also feed information back to the AI: for instance, FountainAI
could query ScoreKit’s model to find out what dynamics are present or how long a section is, aiding the AI’s
reasoning or conversational abilities about the music.
Fountain-Store synergy: All musical changes and creations can be persisted via Fountain-Store,
using the identifiers and semantic tags provided by ScoreKit. This means FountainAI gains a kind
of long-term memory for music. If a user asks “Recall the motif I hummed yesterday”, FountainAI
could search the Store (which contains records of a LilyPond snippet or a ScoreKit object from
yesterday, tagged accordingly) and retrieve it for editing or replay. Fountain-Store’s design
ensures that even as the musical knowledge base grows (potentially thousands of phrases,
scores, lessons), retrieval remains fast and flexible. ScoreKit contributes to this memory
by packaging musical information with metadata, making each entry more searchable by
content (notes, chords) and by concept (mood, technique, tags). In the long run, this integration
imbues the system with a musical semantic memory – the AI can remember and reuse musical
ideas in a coherent way, strengthening the coaching and creative workflow.
UI Integration and Teatro Evolution
On the user interface side, ScoreKit would be the backbone for musical score display and interaction in
Fountain-Coach applications. It naturally extends the Teatro engine, which already laid groundwork for
rendering and even animating visual elements (including music) in Swift. By plugging ScoreKit
into a UI layer (such as a SwiftUI view or web canvas), users could see their music rendered in real time
as they or the AI make changes. This interactive score viewer could support features like: click-to-select
a bar (for applying an AudioTalk phrase to that region), hover to see semantic annotations (e.g., a
13
• 
14
13
• 
15 12
10 11
3

--- Page 4 ---

tooltip showing “  brighter timbre” on a note that had that tag), or scrub playback synchronized to the
notation.  Teatro’s  concept  of  an  Animator and  storyboard  could  even  be  employed  to  highlight
changes – for instance, when an AI adds a crescendo, the UI might briefly highlight those bars to draw
the user’s attention.
Teatro and beyond: In the original Teatro design, GPT-generated LilyPond was immediately
compiled and previewed – ScoreKit takes this to the next level by making the preview live and
editable. It effectively transforms the static PDF generation into an interactive canvas. Moreover ,
ScoreKit’s renderer could output not just to screen but also to SVG or other formats for web
embedding, aligning with Teatro’s multi-format rendering approach (HTML, Markdown, SVG,
image) . Developers can integrate ScoreKit as a Swift package in macOS/iOS apps or use it on
a server to generate notation images on-the-fly for a web client. This versatility means any
Fountain-Coach interface (a coaching app, a web editor , an AR music display) can all rely on the 
same core logic for consistent results. In short, ScoreKit helps present a unified experience:
whether a user is viewing the score, hearing it, or discussing it with the AI, all those modalities
stay in sync through this central notation engine.
Conclusion: Bridging Music, Code, and Language
Implementing  ScoreKit  would  be  a  decisive  step  toward  making  the  AudioTalk  vision  a  reality.  It
provides the connective tissue between symbolic music representation and semantic interpretation,
enabling the system to not only engrave beautiful scores but also understand and manipulate them in
context. With LilyPond integration, musicians and AI get authoritative notation output; with a real-time
Swift renderer , they get immediacy and interactivity; with a semantic-rich format tied into FountainAI
and  midi2sampler ,  every  musical  idea  can  propagate  through  the  system  –  from  plain  language
description  to  written  score  to  actual  sound  –  in  one  cohesive  loop.  ScoreKit  thus  turns  musical
knowledge into something actionable and persistent within Fountain-Coach. By combining notation,
audio, and AI-driven semantics, the Fountain ecosystem moves closer to a future where  language
becomes the API for music, and where creative intent flows effortlessly from the mind to the score
to the ears.
Sources: The  design  draws  on  Fountain-Coach’s  AudioTalk  vision,  existing  Teatro  rendering
prototypes , and the integration of MIDI2.0 and semantic memory across the platform.
These guide the engineering of ScoreKit as a foundational piece of the Fountain-Coach architecture. 
• 
6
10
16
1 2
5 6 14 17
4

--- Page 5 ---

VISION.md
https://github.com/Fountain-Coach/AudioTalk/blob/7c6b7c3390af8e6cd79f975c074cf6966cfcb70a/VISION.md
06_LilyPondMusicRendering.md
https://github.com/Fountain-Coach/Teatro/blob/21d080f70b2c238469bbb0133a5f14b20afdd0ab/Docs/Chapters/
06_LilyPondMusicRendering.md
Addendum.md
https://github.com/Fountain-Coach/Teatro/blob/21d080f70b2c238469bbb0133a5f14b20afdd0ab/Docs/Chapters/
Addendum.md
Verovio | Music notation engraving library for MEI with MusicXML ...
https://www.verovio.org/
SystemArchitecture.md
https://github.com/Fountain-Coach/Teatro/blob/21d080f70b2c238469bbb0133a5f14b20afdd0ab/Docs/
SystemArchitecture.md
VISION.md
https://github.com/Fountain-Coach/Fountain-Store/blob/50654f4d799ff9b5e7bba0c1df57e03c89c56ef1/docs/LEGACY/
VISION.md
1 2 3 4 13 14 16
5 6 7
8
9
10 11
12 15 17
5